= Solution Pattern: Using Change Data Capture for Stack Modernization
:page-layout: home
:sectnums:
:sectlinks:
:doctype: book


This solution pattern brings an architectural solution for scenarios where services integration must happen through data integration and cause no impact to the existing stack.

The architecture demonstrates how *https://www.redhat.com/en/topics/integration/what-is-change-data-capture[Change Data Capture (CDC)]* design pattern and event-driven architectures supports the *extension of existing capabilities with no changes to legacy apps* where new features can be delivered by cloud-native microservices and can deliver with *zero impact new search capabilities* through the integration of legacy and new services with specialized search index services.

// ********************************
// content-overview
// ********************************
[discrete]
== Content overview
====
xref:index.adoc[{counter:module:1}. Solution Pattern]::
+
xref:index.adoc#use-cases[{counter:module1}.{counter:submodule1:1}. Use cases] +
xref:index.adoc#_content_overview[{module1}.{counter:submodule1}. Content overview] +
xref:01-pattern.adoc#_the_story_behind_this_solution_pattern[{module1}.{counter:submodule1}. The story behind this solution pattern] +
xref:01-pattern#_the_solution[{module1}.{counter:submodule1}. The solution] +
xref:index.adoc#_explore_more_solution_patterns[{module1}.{counter:submodule1}. Explore more solution patterns]
+
xref:02-architecture.adoc[{counter:module2:2}. Architecture]::
+
xref:02-architecture.adoc#_common_challenges_when_extending_stack_capabilities[{module2}.{counter:submodule2:1}. Common challenges] +
xref:02-architecture.adoc#tech_stack[{module2}.{counter:submodule2}. Technology stack] +
xref:02-architecture.adoc#in_depth[{module2}.{counter:submodule2}. An in-depth look at the solution's architecture] + 
+
xref:03-demo.adoc[{counter:module3:3}. Demonstration]::
+
xref:03-demo.adoc#_see_the_solution_in_action[{module3}.{counter:submodule3:1}. See the Solution in Action] +
xref:03-demo.adoc#_run_this_demonstration[{module3}.{counter:submodule3}. Run this demonstration] +
xref:03-demo.adoc#_walkthrough_guide[{module3}.{counter:submodule3}. Walkthrough guide]
====

[#use-cases]
== Use cases

Common use cases that can be address with this architecture are:

- Legacy services need to be extended or enhanced but, it is *not* a possibility to implement the changes in the existing application;
- New applications or services must leverage existing data from an existing stack of services;
- The organization seeks to move towards cloud environments and comply to best practices on cloud-native architectures but must also guarantee that any production-active legacy system data is also synchronized to the new services;
- The organization seeks to move towards cloud environments and comply to best practices on cloud-native architectures but must also guarantee that any production-active legacy system data is also synchronized to the new services;
- Legacy applications are now extended and complemented by capabilities delivered by new microservices or services (e.g. search index, cache) but the data should always be synchronized.

// ********************************
// 01-pattern
// ********************************
== The story behind this solution pattern

The solution pattern is backed by the story of a retail store that has been active in the market for more than fifteen years. This retail store uses a single application to manage the `inventory`, `catalog` and `sales` during their daily operations.

[quote, Alex Tizon - CEO, About company current situation]
We are a leading retail store acting in the market for over 15 years. We managed to start digital technologies adoption many years ago by acquiring a solution for our inventory and sales registrations. Our biggest challenge is to join competition with high-tech competitors.

Here are some characteristics of the existing technology stack:

- The technology has been acquired many years ago, and no source code is available.
- The application, application server and databases are installed and running in an on premise environment.
- There are domain experts who know how to keep the inventory and catalog list updated.
- There are operations running 24/7 using the sales application, stopping the application impacts the money income directly.

It's getting hard to compete with more high-tech companies, but the technical team does not have the required skills to boostrap the adoption of cloud-native technologies to enhance the organization online services.

[quote, Avery Smith- CTO, On modernization goals]
Our first step into modernization is developing a new cashback system for our customers. Adding to that, our current service lacks good searching capabilities so we need better ways for customers to find what they need.

[discrete]
=== Story Goals

To get started, the business and technical executives came up with initial goals to be achieved as a first step towards the organization modernization:

The Cashback Wallet Functionality::
- Customers should receive a percentage of their purchases as cashback.
- All the earned cashback is now part of the Cashback Wallet.
- With this, upcoming purchases can be paid either with the customer money or using available cashback in the customer Cashback Wallet.
- The customer should be able to check all transaction history including earned cashback and spent cashback
- New services and technologies must easily fit in cloud environments as we seek to move forward with clo adoption.

Enhanced search capabilities::
- Customers should be able to easily search for what they need;
- They should be able to search by part or all of the product name.
- The operations should still be able to use the legacy application to maintain the catalog and this data should benefit from the enhanced search capabilities.
- The new solution should be able to provide to technical team metrics about the search, like query latency and  request rate;
- If a new tool is used, we want to be able to choose where to deploy it - we want to be free of vendor lock-in.

== The Solution

This solution pattern builds on top an event-driven architecture in order to support the extension of the legacy stack. The architecture includes new microservices, event streaming, event processing and search indexing tools.

In respect to the xref:_story_goals[story goals] and xref:use-cases[targeted use cases], it's recommended to consider adopting an https://www.enterpriseintegrationpatterns.com/[Enterprise Integration Pattern] for data integration, more specifically, adopting the https://www.redhat.com/en/topics/integration/what-is-change-data-capture[Change Data Capture (CDC)] pattern.

This solution requires *no source code changes* in the existing services. The core concept builds on data integration between legacy and new services through usage of asynchronous events. A short description of this solution key concept is:

****
Relevant changes to data persisted in the tracked databases (e.g. delete/insert/update) are captured and published as events. Then, external services react to events and execute necessary operations.
****

The integration happens like this:

1. Using https://debezium.io/[Debezium], the database becomes an event stream. Since data changes are directly tracked, the legacy application code won't require changes.
2. The captured data changes are pushed to topics in a https://www.redhat.com/en/topics/integration/what-is-apache-kafka[Kafka] broker.
3. The services that offers that extra capabilities can then subscribe to relevant topics and use the events to obtain the information needed to execute its logic.

[TIP]
For detailed architecture diagrams please check the xref:02-architecture.adoc[In Depth Architecture]] section.

See below a simplified representation of the solution:

.Simplified representation of the integration between the legacy application and the new technology stack.

image::01/simplified-tech-usage.png[width=100%]

= Solution Pattern: Using Change Data Capture for Stack Modernization
:sectnums:
:sectlinks:
:doctype: book

// ********************************
// 02-architecture
// ********************************
= Architecture 

Introduction for the architecture of this solution pattern.

== Common Challenges when Extending Stack Capabilities

To better explain and detail the reasons for the existence of this solution pattern we'll picture some common needs and challenges amongst organizations that already have production systems and seeks innovation and modernization.

=== Distributed Systems and Data Access

In a distributed system it's common to have services that must use data owned by other services to deliver its capabilities.

====
*First challenge*

Currently, there is a production legacy *retail service* that persists all sales and inventory data in a single database. The challenge is now to deliver a *cashback* capability that is highly dependent on the retail data, leveraging modern technology and architecture design best practices.
====

At a first glance, a simple solution to such complex problem would be to implement the cashback service with its own database for cashback domain data, and directly accessing retail database to obtain and update sales information.

image::01/incorrect-db-access.png[width=75%]

Unfortunately, this is an anti-pattern for data access and management in a distributed architecture. Multiple services should not consume and change data directly in databases owned by other services.

=== The need to store data in multiple data stores

Another modernization challenge is enhancing search capabilities in huge set of data, improving efficiency by increasing search response time, reducing number of disk accesses, using efficient search algorithms and being able to scale according to demand. To address such problem, we could complement the retail service by adding a search index like https://www.elastic.co/[Elasticsearch].

====
*Second challenge*

In other to start consuming search capabilities from tools like Elasticsearch, the first step is to feed data into the tool's index. This process is called `indexing`. All the queryable data needs to be pushed to the tool's storage, the index (Apache Lucene).

The production stack is based on the *retail service* that currently persists data to a single database. The challenge is to make all the retail data searchable through a tool like Elasticsearch.
====

One could think about changing the service to push the data not only to its own database, but also to elasticsearch. It becomes a distributed system where the core data operations are no longer handled in single transactions. Be aware: this is yet another anti-pattern, called https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications[dual write].

[IMPORTANT]
https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications[Dual writes] can cause data inconsistency problems for distributed systems.

image::01/incorrect-dual-write.png[width=75%]

The consequence of issues in this solution would be to have an outdated data being queried by the user, in other words, a user could potentially see an item for sale that is no longer available, or see a list of items with an outdated price.

Other than data inconsistency, changes to the legacy application would be required. Such changes are not always possible either for business or technological restrictions.

[.anti-patterns]
==== Avoid Antipatterns

Think twice before delivering solutions with antipatterns. Here's a summary of the two antipatterns we've seen so far:

Shared databases::
Multiple services are linked through a single database.
Dual write::
A situation when a service inserts and/or changes data in two or more different data stores or systems. (e.g. database and search index or a distributed cache).

[#tech_stack]
== Technology Stack

* https://www.redhat.com/en/technologies/cloud-computing/openshift[Red Hat OpenShift]
* Red Hat Application Foundation
** https://access.redhat.com/products/quarkus[Quarkus]
** https://www.redhat.com/en/technologies/jboss-middleware/fuse[Camel (a.k.a. Red Hat Fuse)]
** https://developers.redhat.com/articles/2021/12/06/improve-your-kafka-connect-builds-debezium[Debezium and Kafka connect]
** https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka[Kafka (a.k.a. Red Hat AMQ Streams]
** https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka[Kafka Streams]
* Other:
** https://www.elastic.co/[ElasticSearch]
** https://www.postgresql.org/[PostgreSQL database]
** https://helm.sh/[Helm]


[#in_depth]
== An in-depth look at the solution's architecture

The whole solution builds upon the event streams flowing for each change on the database. The data integration is the enabler for all the new services to execute their respective operations.

The following https://c4model.com[diagram] represents an abstract architectural view of the system scope, personas involved, the multiple apps and storage:

.Architecture Diagram: System Context. An abstract representation of the whole solution.
[link=_images/02/architectural-overview.png, window="_blank"]
image::02/architectural-overview.png[width=100%]

Three main application contexts are part of this architecture. The *retail application* represents the legacy application. The *cashback application* and the *search application*, represent the two new use cases to be addressed without impacting the existing service.

The two base scenarios targeted are, first, the event-driven processing of cashback for every customer purchase according to his/her customer status, and second, allowing the usage of full-text search capabilities for data that is still maintained via legacy application.

[#scenario-cashback-wallet]
=== Scenario: Cashback Wallet

// ********************************
// partials/_02-architecture-cashback-scenario.adoc[]
// ********************************
a) *Cashback Wallet:* A new microservice implements new capabilities enabled by data integration. This integration happens via database event streaming and processing from legacy database to the new cashback database.

.Architecture Diagram: Cashback Wallet Context. A representation of the solution for cashback functionality.
[link=_images/02/arch-cashback-overview.png, window="_blank"]
image::02/arch-cashback-overview.png[width=100%]

1. The cashback processing kicks-off when a new purchase is registered via legacy application. In the demonstration implemented for this solution pattern, we use a service to simulate purchases and register them in the database.
2. Debezium will capture all changes in the database tables below;
- List of tracked tables in retail database: `public.customer`,`public.sale`,`public.line_item`,`public.product`
3. Next, https://debezium.io[Debezium] streams the data them over to Kafka. The event streaming solution can be hosted on-premise or on the cloud. In this implementation, we are using https://red.ht/TryKafka[Red Hat Managed OpenShift Streams for Apache Kafka].
4. An integration microservice, `sales-streams`, reacts to events captured by Debezium and published on three topics, respective to `sale-change-event` and `lineitem-change-event`.
5. Using https://quarkus.io/guides/kafka-streams[Kafka Streams], the service aggregates multiple events that correlates to a unique purchase. The service will calculate the total amount of the purchase based on individual items price captured, and will publish the enriched data to the topic `sales-aggregated`.
6. Another event-driven microservice is responsible for tracking customer's change streamed by Debezium, and for reacting to new enriched sales information - in other words, reacting to data processed by the `sales-stream` application.
7. The service synchronizes `customers` and `expenses` in the cashback database. This database used to store new cashback feature-related data.
8. Once the `cashback-connector` microservice finished its operations, it will notify the ecosystem that a new or updated expense is available - especially for cashback-processing. A new event is published to an `expense-events` topic so that interested (subscribed) services can act if needed.
9. Now that every information is synchronized in the cashback database, the system can calculate and update any incoming cashback amount the customer earned when purchasing products. The choreography goes on as the `cashback-service` jumps in and reacts to the `expense-events` topic.
- This microservice is reponsible for the calculation of the cashback based on a customer status, and for making sure the customer will earn a percentual relative to each expense amount. Every customer owns a *Cashback Wallet*, in other words, all incoming cashback can be accumulated and used later. Since this service is responsible for integrating services in a cloud environment, the  technologies used in the demo implementation are https://quarkus.io/guides/camel[Camel, with Quarkus as the runtime].
10. With the values properly calculated, the `cashback-service` persists cashback-related information, including new cashback wallets for first-time customers, incoming cashback for each single customer's expense, and total cashback.
11. The user can visualize cashback data using a sample application `cashback-ui`, which runs with Quarkus and uses Panache Rest to handle persistence and expose REST endpoints. Information is finally displayed through an angular-based page. This application is used in the demo to help developers visualizing the demonstration results.
+
.Cashback Wallet UI: sample demo ui for easier data visualization when trying the solution pattern implementation.
[link=_images/02/cashback-ui.png, window="_blank"]
image::02/cashback-ui.png[width=100%]

[#scenario-search]
=== Scenario: Full-text search for data in legacy database
// ********************************
// partials/_02-architecture-search-scenario.adoc
// ********************************

b) *Full-text search of legacy data:* enables full-text search for legacy data by adopting data integration through event streaming and processing. All changes to the legacy database tracked tables, including the operations create, updated and delete, should be reflected in the search index tool. The indexing tool will then store and index data in a way that supports fast searches.

.Architecture Diagram: Search Solution Context. A representation of the solution for the new search functionality.
[link=_images/02/arch-search-overview.png, window="_blank"]
image::02/arch-search-overview.png[width=100%]

Similarly to the behavior of the cashback scenario, here Debezium is tracking changes in the retail database. All changes to product data is streamed to Kafka. The `elastic-connector` service reacts to product events and synchronizes it within ElasticSearch product index.

For demonstration purposes, the `search-service` holds a sample UI to allow searching data in the indexing tool.

The following services are part of this scenario:

* *Retail database*: stores all information from the legacy application. It includes information about *products*, *customers* and new *sales* (detailed through *line items*).The tables in this database are tracked by Debezium.
* *Debezium*: tracks all events that happens in tables from retail db (public.customer,public.sale,public.line_item,public.product) and streams changes into Kafka streams;
* *Elastic connector service*: an event-driven microservice that reacts to products' events and push relevant updates to Elastic. This service capabilities were developed with with Camel and Quarkus.
* *Search service*: a sample quarkus service that integrates with ElasticSearch using the https://quarkus.io/guides/elasticsearch[quarkus elastic-rest-client extension], and exposes a REST endpoint for searching products by name and description. For demonstration purposes, this service has a page to facilitate visualizing the search results.

.Seach Service: a Quarkus client that integrates with Elastic for easier search results visualization.
[link=_images/02/search-ui.png, window="_blank"]
image::02/search-ui.png[width=100%]

// ********************************
// 03-demo
// ********************************


== See the Solution in Action

This section brings information about the implementation of the solution pattern, how you can install it, try it out and check the details of the implementation with an actual running application.

Here's a list of videos that you can use to explore this solution pattern.

* xref:03-demo.adoc#_see_an_overview_and_demonstration_of_this_solution_pattern[Solution Pattern Overview]
* xref:03-demo.adoc#_see_the_provisioning_in_action[How to provision this demo]
* xref:03-demo.adoc#_see_the_search_feature_in_action[The enhanced search capability in action]
* xref:03-demo.adoc#_see_the_cashback_wallet_in_action[The Cashback Wallet capability in action]

[#_see_an_overview_and_demonstration_of_this_solution_pattern]
See an overview and demonstration of this solution pattern:

Check below a twenty minutes explanation and demonstration of this solution pattern:

video::vTdP2mLXiHg[youtube, width=800, height=480]

== Run this demonstration

In order to try out this demonstration you will need to provision the environment. From an overall perspective, these are the steps to provision the demo:

1. Log in to OpenShift with `cluster-admin` role;
2. Create an OpenShift Streams instance, configure ACL and topics;
3. For ansible, configure the set of variables pointing with your environment settings;
4. Run the ansible playbook and enjoy the demo.

=== See the provisioning in action 

The video below demonstrates how to do the provisioning that is described in details the next sections. You can follow the provisioning steps as you follow the video.

video::TvrbX4gKiv0[youtube, width=800, height=480]

=== Pre-requisites
==== Preparing the local environment 

Here is the list of tools you need in your machine to so you can use the automated installation.

TIP: For a better experience during provisioning and demo usage, it's recommended to have these CLI tools installed locally.

* https://docs.openshift.com/container-platform/4.10/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli[OpenShift CLI (oc client)]
* https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html[Ansible CLI] (_tested with v2.9_)
** Ansible https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html[kubernetes.core] module
* __(optional)__ https://github.com/redhat-developer/app-services-guides/tree/main/docs/rhoas/rhoas-cli-installation#installing-the-rhoas-cli[RHOAS CLI] for OpenShift Streams management.

To check if you have the cli tools, you can open your terminal and use following commands:

[.console-input]
[source,shell script]
```
oc version #openshift cli client
ansible --version 
ansible-galaxy --version 
ansible-galaxy collection list #the list should include kubernetes.core
```

If you can't see `kubernetes.core` collection listed, you can install it with `ansible-galaxy`:

[.console-input]
[source,shell script]
```
$ ansible-galaxy collection install kubernetes.core
```

*Optional: Managing OpenShift Streams using a CLI tool*

It is possible to do all interaction with your OpenShift Streams managed service (kafka) via the web console. If instead you like using the terminal and want to use a CLI tool, you will need *rhoas cli*. This cli allows interaction with Red Hat OpenShift Application Services like the OpenShift Streams kafka we will use.

You can find a straightforward installation guide for multiple OS at https://github.com/redhat-developer/app-services-guides/tree/main/docs/rhoas/rhoas-cli-installation#installing-the-rhoas-cli[Installing the RHOAS CLI].

==== Preparing the platforms

* OpenShift cluster (version >= 4.9) with _cluster-admin_ privileges.
+
TIP: If you have access to rhpds, you can request and use an `OpenShift 4.10 Workshop` enviroment.
+

* Access to OpenShift Streams for Apache Kafka.
+
TIP: If it's your first time using OpenShift Streams, don't worry. It's a zero-cost service for developers and everyone can try it out. You can register and order your instance at https://red.ht/TryKafka[https://red.ht/TryKafka].

=== Provisioning the demo

The solution's components and services can be automatically provisioned using an ansible playbook.

The following steps will guide you on setting up an instance of OpenShift Streams for Apache Kafka and its resources, plus provisioning the demo services using Ansible.
[#cli-tools]

==== Provisioning OpenShift Streams (Kafka) 

Before moving ahead to the steps of provisioning the services within your OpenShift cluster, first you should provision and configure your Kafka instance.

TIP: If you need detailed instructions on how to provision, configure and operate of your managed Kafka instance, please check this step-by-step https://redhat-scholars.github.io/managed-kafka-workshop/managed-kafka-workshop/main/01-getting-started.html[Getting Started with OpenShift Streams for Apache Kafka] guide.

See below a straightforward guide to create and your instance:

1. Navigate to https://console.redhat.com and log in with your Red Hat Account ID;
1. Select the *Service Account* menu and create a new Service Account to connect to your Kafka instance;
+
IMPORTANT: Take note of the service account id and password, you'll need both information during the provisioning.
+
1. Next, in the left menu, select *Application and Data Services -> Streams for Apache Kafka -> Kafka instances*;
1. Create a new Kafka instance;
- Use a name of your choice. You can use the default values for creating the instance.
1. Once your instance is ready, click on the instance and open the "connection" tab. take note of the following data:
- Bootstrap server (e.g. cdc-kafka-caah-ekucfsh--lhhsqa.bf2.kafka.rhcloud.com:443)
+
image::03/kafka_instance_connection_info.png[]
+
1. Configure the ACL for your Service Account. The Service Account should have the following permissions:
+
* `read`, `write`, `create` permissions for all topics
* `read` permissions for all consumer groups
* If you have `rhosak` CLI installed, you can execute the following commands to login to the service, select your kafka instance and add the proper configuration, *replacing `srvc-acct-9999` with your service client id*:
+
IMPORTANT: If you do not use the right service account id, the deployed services will throw an authentication error.
+
[.console-input]
[source,shell script]
```ssh
rhoas login
rhoas kafka list 
rhoas kafka use
rhoas kafka acl grant-access --producer --consumer --service-account srvc-acct-9999 --topic all --group all -y
```
+
1. Create the following topics, *all with 1 partition*:
* `retail.sale-aggregated`
* `retail.expense-event`
* `retail.updates.public.line_item`
* `retail.retail.updates.public.sale`
* `retail.updates.public.customer`
* `retail.updates.public.product`
+
image::03/kafka_instance_topics.png[]
+
* if you are using `rhoas cli`, you can create the topics with these commands:
+
[.console-input]
[source,shell script]
```ssh
rhoas kafka topic create --name=retail.sale-aggregated --partitions=1
rhoas kafka topic create --name=retail.updates.public.customer --partitions=1
rhoas kafka topic create --name=retail.updates.public.product --partitions=1
rhoas kafka topic create --name=retail.updates.public.sale --partitions=1
rhoas kafka topic create --name=retail.updates.public.line_item --partitions=1
rhoas kafka topic create --name=retail.expense-event --partitions=1
```

==== Installing the demo

This solution pattern offers an easy installation process through ansible automation and helm charts. To get your environment up and running, follow the steps below:

1. Clone the repository below to your workstation
+
[.console-input]
[source,shell script]
```shell
git clone https://github.com/solution-pattern-cdc/ansible.git
cd ansible
```
+
1. Copy the `inventories/inventory.template` file to `inventories/inventory`;
1. Remember the OpenShift Streams values we took note? It's time to use them. In the `inventories/inventory` file, provide the connection details for your Kafka instance:
* **rhosak_bootstrap_server**: Bootstrap server of your managed Kafka instance;
* **rhosak_service_account_client_id**: Client ID of your Service Account;
* **rhosak_service_account_client_secret**: Client Secret of your Service Account;
1. Run the Ansible playbook:
+
[.console-input]
[source,shell script]
```sh
ansible-playbook -i inventories/inventory playbooks/install.yml
```

Once the playbook finished successfully, you should be able to see the different components of the demo installed in the `retail` namespace on your OpenShift cluster.

To check if your environment is healthy:

1. Access your OpenShift console, and using the Administrator view, on the left menu select *Workloads -> Deployments*;
2. All services should be healthy, like displayed below:
+
image::03/ocp_pods_running.png[]

=== Obtaining services' URL

You can access the three services that exposes a UI through the exposed routes. Use one of the two options below to get the routes:

a. Using `oc cli`, copy and paste the whole command below:
+
[.console-input]
[source,shell script]
``` 
cat << EOF
========================================
Kafdrop: https://$(oc get route kafdrop --template='{{ .spec.host }}' -n retail) 
Search service: https://$(oc get route search-service --template='{{ .spec.host }}' -n retail) 
Simulation Service: https://$(oc get route retail-simulation --template='{{ .spec.host }}' -n retail)/q/swagger-ui 
Cashback Wallet UI: https://$(oc get route cashback-service-ui --template='{{ .spec.host }}' -n retail)
========================================
EOF
```
+
b. Using the OpenShift console:
+
image::03/ocp_routes.png[]

== Walkthrough guide

A retail store specialized in plants wants to grow its market by expanding in the online market. To do so, they need to start the adoption of new technologies without impacting the existing application that is currently running in production. All information about sales, customers and products are still maintained via legacy application, but this data is also required by the new capabilities.

Two new functionalities are now part of the retail solution:
1. Enhanced search capabilities for products
1. Cashback wallet for customers

Both solutions are build on top of an event driven architecture, which means that all services are integrated with an orchestration where each one execute its own operations when relevant events are published in the ecosystem.

Let's see both solutions in action, starting with the new search capabilities.

=== Enhanced search capabilities for products

To test the enhanced search capabilities, we will:

1. Use the `search service` to see existing data that is available in the ElasticSearch index;
2. Add a new product directly to the `retail database` (legacy), to check the ecosystem behavior;
3. Confirm that the new product shows up in the search;
4. Check the events that were published in order for the synchronization to happen;
4. Delete the product directly on the retail database;
4. Confirm that the product no longer shows up in the `search service`.

==== See the search feature in action

In this video you can see the working implementation of the new enhanced search capabilities:

video::C90x_utWQkk[youtube, width=800, height=480]

==== Trying out the new enhanced search

1. Using your browser, open the `search service`.
+
NOTE: You can get the URL as described in the section xref:03-demo.adoc#cli-tools[obtaining the services URL].
+
2. In the search field, search for "*yellow*". You should see several results.
+
image::03/search-service-result-yellow.png[]
+
2. Next, search for "kopi" or "java. No result will show up.
3. Let's insert a new product directly in the `retail-db` and see if it will reflect on this service. Use the console inside the `retail-db` container. You can either access the container using your browser, accessing the OpenShit Console (*Workloads -> Pods -> retail-db-XXXX -> Terminal*);
+
Or by using your terminal as shown below:
+
```
oc project retail
oc rsh deployment/retail-db
```
5. Next, inside the container, we will access postgres, connect to the `retail` database and check the structure of the `product` table:
+
```
psql
\c retail
\d product
```
+
As we can see, a product has an `id`, `name`, `description` and a `price`.
+
image::03/retail-db-terminal.png[]
+
1. Let's add a new product in this table, the product "*Kopi luwak*":
+
```sql
insert into public.product (product_id, name, price, description) values (7777, 'Kopi luwak', 20, 'Kopi luwak is a coffee that consists of partially digested coffee cherries, which have been eaten and defecated by the Asian palm civet (Paradoxurus hermaphroditus). It is produced mainly on the Indonesian islands of Sumatra, Java, Bali, Sulawesi, and in East Timor.');
```
+
1. Now, as required by the use case, even though this data was changed in the legacy database, it should be available for search in the new services. Let's confirm that this change was reflected in the ElasticSearch products index.
+
Open the `search-service` application in your browser and search for "java" or "kopi". You should be able to see your new product.
+
image::03/search-service-result-java.png[]

Let's delete this product from the retail database to validate if delete operations are also being tracked.

1. In the `retail-tb` container terminal, now execute the following SQL:
```sql
DELETE FROM public.product where product_id = 7777;
```
2. Go back to the `search-service` in your browser, and search for '*kopi*' or '*java*' again.

==== Looking behind the scenes - enhanced search

It's now time to take a look at how the system is working in order to allow this capability to work as we have seen.

The components of the search capability we have just tried are:

[cols="28m,^.^13,~"]
[frame=all, grid=all]
|===
|*Service* | *Type* | *Description*
| retail-db
| PostgreSQL database used by the legacy services;
| Persistence

|kafka-connect-connect
.2+| Integration
| Kafka connectors for database event streaming (debezium);

|elastic-connector
|Camel + Quarkus service for event-driven synchronization of product data with ElasticSearch;

|kafdrop
.2+| Data Visualization
|a kafka client ui to facilitate the visualization of events and topics;

|search-service
|Quarkus + ElasticSearch extension to simplify the visualization of the indexed data residing in elastic search;
|===

NOTE: If you go to your OpenShift, you should be able to see one `deployment` resource for each of the above services.

*So, how was the new product added to the ElasticSearch index?*


1. A new product is created in the `retail.product` table, in the legacy database `retail-db`;
2. xref:appendix-a.adoc#_kafka_connect__debezium_installation[Debezium] tracks it and publishes the events it to topics in OpenShift Streams;
3. The `elastic-connector`, implemented with Camel and Quarkus is subscribed to the topic mentioned above. It processes the event data and pushes the *product name* and *description* to an ElasticSearch index:

.Partial code - processing logic in the https://github.com/solution-pattern-cdc/elastic-connector/blob/main/src/main/java/org/acme/retail/ProductRoute.java[`ProductRoute`]

[.console-input]
[source,java]
----
(...)
    .process(exchange -> {
        Message in = exchange.getIn();
        JsonObject after = new JsonObject(in.getBody(Map.class)).getJsonObject("after");
        Map<String, String> document = new HashMap<>();
        document.put("name", after.getString("name"));
        document.put("description", after.getString("description"));
        IndexRequest request = new IndexRequest(in.getHeader(ElasticsearchConstants.PARAM_INDEX_NAME, String.class))
                .id(String.valueOf(after.getLong("product_id"))).source(document);
        in.setBody(request);
    })
(...)
----

This flow can be represented like this:

image::03/arch_search.png[]

=== Cashback Wallet functionality

Now, let's see more ways we can explore CDC to add new capabilities to our existing stack. Since we have all the new sales being streamed as events, we can use it to build the new cashback wallet business.

To walk through this demonstration, you will need to access the following services in your browser:

* cashback-service-ui
* kafdrop
* simulation service Swagger-UI

=== See the Cashback Wallet in action 

The following video shows the working implementation of the new cashback wallet capabilities:

video::W813zm5qG2Q[youtube, width=800, height=480]

==== Trying out the new cashback wallet 

1. Open the `cashback-service-ui`:

+
NOTE: You can get the URL as described in the section xref:03-demo.adoc#cli-tools[obtaining the services URL].
+
2. You should be able to see a list of cashback wallets and its data:
+
image::03/cashback-wallet-clean.png[]
+
3. Choose one of the customers in that list that has no cashback. It will be easier ot see the new cashback credits. You can see the customer ID in the beggining of the line:
+
image::03/cashback-wallet-customer-id.png[]
+
4. Next, we will simulate as if a customer has purchased five items in the store. In your browser open the `simulation service` swagger-ui, (service-url/q/swagger-ui).
+
image::03/simulate-purchase.png[]
+
5. Click on `try it out`, input the customer ID you have chosen, and submit the request. This will generate five purchases for this customer.
+
image::03/simulate-purchase-result.png[]
+
6. You should get an HTTP 200 result. In the legacy system, the purchases are stored in two different tables, the `retail.sale` and `retail.line_item`. So if you simulate five sales, the data will be stored in both tables and streamed as events by Debezium to two respective topics.
+
Through a series of orchestrated operations, the data will be aggregated, processed, and enriched (`sales-aggregated` service), to finally be used to calculate and update the cashback wallet's values (`cashback-service`).
8. Open Kafdrop in your browser.
8. Locate and click on the topic `retail.sale-aggregated`, and then, click on *view messages*. This is the result of the Kafka Streams (`sales-stream` service) operations of aggregation, processing and enrichment of the events' data that were streamed by Debezium:
+
image::03/kafdrop-sales-aggregated-messages.png[]
+
NOTE: To see a detailed explanation about the events processing refer to the xref:_looking_behind_the_scenes__cashback_solution[Looking behind the scenes] section.
+
8. Open the Cashback Wallet in your browser and refresh the page. You should be able to check the new earned cashback for each purchase of your customer!
+
image::03/cashback-wallet-complete.png[]

See below a diagram that represents the orchestration processing that just happened when you simulated new purchases and saw the respective incoming cashback:

image::03/arch-cashback.png[]

==== Looking behind the scenes - cashback solution

Differently than the search capability that only requires the integration layer (Retail DB -> ElasticSearch), to create cashback wallets we'll need to process and enrich the data before we use it. We will also need to guarantee the synchronization between the customer data in the `retail-db` and the `cashback-db`.

1. When a new sale is registered, new lines are created in the `retail.sale` and `retail.line_item` tables.
2. Debezium then tracks and publishes events to *two topics*, one for each respective table, and one event for each respective line added/updated event that was tracked. But notice that in order for us to apply the cashback calculation business logic, we'll have in mind good design and architecture practices for microservices, where each microservice "is supposed to do one thing, and do it well". So, the event data aggregation, processing and enrichment will be executed by a service (`sales-streams`) before we actually do the cashback operations in another service (`cashback-service`);
+
Here's another way to explain this:
+
* if someone buys two cactus and one lilly in the same purchase, there will be two line_items registered for a single sale. See below the tables structures:
+
[.console-input]
[source,sql]
----
$ oc rsh deployment/retail-db #<1>
sh-4.4$ psql #<2>
psql (12.5)
Type "help" for help.

postgres=# \c retail #<3>
You are now connected to database "retail" as user "postgres".
retail=# select * from sale;  #<4>
sale_id | customer_id |          date
---------+-------------+-------------------------
1000 |        1000 | 2022-06-03 20:27:57.66
1001 |        1000 | 2022-06-03 20:27:57.767
1002 |        1000 | 2022-06-03 20:27:57.852
1003 |        1000 | 2022-06-03 20:27:57.854
1004 |        1000 | 2022-06-03 20:27:57.857
(5 rows)

retail=# select * from line_item; #<5>
line_item_id | sale_id | product_id | price  | quantity
--------------+---------+------------+--------+----------
1000 |    1000 |        198 |  99.40 |        2
1001 |    1000 |        851 |  72.97 |        3
1002 |    1000 |         87 |  66.19 |        3
1003 |    1000 |        243 |  83.20 |        1
1004 |    1001 |         80 | 127.56 |        3
1005 |    1001 |        639 | 193.80 |        1
1006 |    1002 |        563 | 156.08 |        3
1007 |    1003 |        532 |  89.98 |        3
1008 |    1003 |        374 |  87.17 |        1
1009 |    1003 |        932 |  32.69 |        3
1010 |    1003 |        662 | 141.31 |        3
1011 |    1003 |        304 |  39.84 |        1
1012 |    1004 |        138 | 125.81 |        3
1013 |    1004 |        656 | 103.99 |        3
1014 |    1004 |        285 | 168.79 |        3
1015 |    1004 |         84 | 113.79 |        2
(16 rows)
----
<1> Use `oc-client` to access the `retail-db` container;
<2> Access PostgreSQL from within the container;
<3> Connect to the retail database;
<4> List all the sales;
<5> List all the items of the sales;
+
* Debezium will stream each change individually, which results with several events in two topics, one of each table.
* But, when we calculate the earned cashback for the sale, we use the total amount of the sale - the sum of all the line items of that sale.
* Using https://developers.redhat.com/learn/openshift-streams-for-apache-kafka/guided-workshop-for-kafka-streams/what-is-kafka-streams[*Kafka Streams*], the `sales-aggregated` service aggregates, processes and enriches the events' data.
+
.Partial code in the `Sales-Streams` service used to aggregate and enrich data;
[.console-input]
[source,java]
----
// Join LineItem events with sale events by foreign key, aggregate Linetem price in sale
KTable<Long, AggregatedSale> aggregatedSales = lineItemTable
        .join(saleTable, lineItem -> lineItem.sale,
                (lineItem, sale) -> new SaleAndLineItem(sale, lineItem),
                Materialized.with(Serdes.Long(), saleAndLineItemSerde))
        .groupBy((key, value) -> KeyValue.pair(value.sale.saleId, value), Grouped.with(Serdes.Long(), saleAndLineItemSerde))
        .aggregate(AggregatedSale::new, (key, value, aggregate) -> aggregate.addLineItem(value),
                (key, value, aggregate) -> aggregate.removeLineItem(value),
                Materialized.with(Serdes.Long(), aggregatedSaleSerde));

aggregatedSales.toStream().to(aggregatedSaleTopic, Produced.with(Serdes.Long(), aggregatedSaleSerde));
----
+
8. Next, if you go back to the homepage of Kafdrop, open *`retail.expense-event` -> view messages -> view messages*; The `sales-streams` service to notify the ecosystem that new processed information is available by publishing events on the `expense-event` topic.
+
Let's see the result of this processing with Kafdrop.
+
image::03/kafdrop-expense-event.png[]
+
Based on these events published in the `expense-event`, services like the `cashback-service` can react and use the event data to handle the cashback business logic operations.
+
NOTE: See how the values are calculated and persisted in the https://github.com/solution-pattern-cdc/cashback-service/blob/main/src/main/java/org/acme/cashback/processor/ValuesProcessor.java[cashback values processor] in the `cashback-service`
+
* Let's take a look over the cashback service processing:
+
.Partial code implementation in the https://github.com/solution-pattern-cdc/cashback-service/blob/e116b0b0f8067c1a69298e6e4b214224c0d3e1b6/src/main/java/org/acme/cashback/route/CashbackRoute.java[Cashback Route] in the cashback-service.
[.console-input]
[source,java]
----
        from("kafka:{{kafka.expenses.topic.name}}?groupId={{kafka.cashback_processor.consumer.group}}" + #<1>
                "&autoOffsetReset=earliest")
                .routeId("CashbackProcessor") 
                .unmarshal(new JacksonDataFormat(ExpenseEvent.class))
                .setHeader("operation", simple("${body.operation}")) #<2>
                .setHeader("sale_id", simple("${body.saleId}")) #<2>
                .to("direct:filterInvalidOperationCodes") #<3>
                .to("direct:getData") #<4>
                .to("direct:filterInvalidData") #<5>
                .choice()
                .when().simple("${header.operation} == 'c'").log(LoggingLevel.DEBUG,"Processing create event") #<6>
                    .process("valuesProcessor")
                    .choice()
                        .when(simple("${body.cashbackId} == null"))
                            .log(LoggingLevel.DEBUG, "No cashback wallet exists. Creating new cashback for: ${body}")
                            .to("direct:createAndPersistCashback") 
                    .end()
                    .to("direct:updateEarnedCashbackData")
                .endChoice()
                .otherwise().when().simple("${header.operation}== 'u'").log(LoggingLevel.DEBUG,"Processing update event") #<7>
                    .process("valuesProcessor")
                    .to("direct:updateEarnedCashbackData")
                .end();
----
<1> Consumed topic with name configured in the property `kafka.expenses.topic.name`;
<2> Sets incoming information in the message header;
<3> Filter out incoming operations that are not `create` and `update`;
<4> Retrieves existing customer and cashback information from the local database for the incoming sale;
<5> Filter out information for incoming data that is invalid - is not in the cashback database;
<6> When a new expense "create" event is received, the service checks if the customer already has a wallet - if not, creates one. Then, it updates the cashback wallet values calculated and persisted.
<7> If the incoming operation is "update", then, a new wallet does not need to be created. The values are calculated and updated.


== Conclusion 

In this section you have learned how to:

. Provision the demo environment;
. How to try out and check how CDC enables the delivery of the demo implementation:
.. How a new search index technology could be added to the existing solution and enable enhanced search capabilities for legacy data;
.. How a whole new cashback wallet capability could be added without impacting the legacy systems by using a distributed, event-driven and microservice-based architecture;
. Learn in-depth details about services can be orchestrated;

The solution is built on top of a hybrid cloud model, with containerized services running on OpenShift (can be on a private or public cloud depending on how you provision the demo) consuming a managed OpenShift Streams for Apache Kafka. OpenShift streams is heart of this solution - it's a resilient and highly available Kafka instance managed by Red Hat, where all the topics reside and where all services can receive and send all events from/to.

This design is only possible by the designing the architecture based on the Change Data Capture pattern - which was delivered with Debezium and Kafka Connectors.